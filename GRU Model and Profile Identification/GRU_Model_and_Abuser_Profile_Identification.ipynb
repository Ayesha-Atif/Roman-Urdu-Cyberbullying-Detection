{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d702fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tweets = pd.read_csv(\"Data_With_Profiles.csv\")\n",
    "labels = tweets['Label']\n",
    "\n",
    "# Preprocess the tweets (optional)\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "\n",
    "preprocessed_tweets = [preprocess_text(tweet) for tweet in tweets['Tweet_Text']]\n",
    "\n",
    "# Tokenize and pad the tweets\n",
    "max_vocab_size = 10000\n",
    "max_sequence_length = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocessed_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_tweets)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "num_classes = len(np.unique(labels))\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Create a GRU model using Keras\n",
    "embedding_dim = 32\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_vocab_size, embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(64, return_sequences=True),\n",
    "    GRU(32),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1_score])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_one_hot, epochs=10, validation_data=(X_test, y_test_one_hot))\n",
    "\n",
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "accuracy_values = history.history['accuracy']\n",
    "val_accuracy_values = history.history['val_accuracy']\n",
    "\n",
    "# Evaluate the model\n",
    "# Assuming X_test and y_test_one_hot are already prepared\n",
    "loss, accuracy, precision, recall, f1 = model.evaluate(X_test, y_test_one_hot)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation loss and accuracy results visualization\n",
    "\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "plt.plot(epochs, loss_values, 'b', color='red', label='Train')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='blue',label='Test')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, accuracy_values, 'b', color='red', label='Train')\n",
    "plt.plot(epochs, val_accuracy_values, 'b', color='blue', label='Test')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing of trained model over some random tweets not in the dataset\n",
    "\n",
    "new_tweets = [\n",
    "    \"Tum achi lg rhi ho\",\n",
    "    \"Tum Pathan ho tumhen kya pta aqal kya hoti ha\", \n",
    "    \"yahoodi saazish aurat march\", \n",
    "    \"Tum hindu ho, tm log hote he haraami ho\",\n",
    "    \"lo ji qadiani yahoodi agent any aik dafa phir baazi maarli musalmaan apni apni siyaasat kartay reeh gaey\", \n",
    "    \"hijra ha ye\", \n",
    "    \"randdi ki olaad\",\n",
    "    \"chal tu mat bhaunk\",\n",
    "    \"han theek ha\", \n",
    "    \"bakwas band kr tu bhi chali ja\", \n",
    "    \"ye chutiyaapa myth hi faila raha ha\", \n",
    "    \"oye bahanchod yih kya chutiyaapa h\", \n",
    "    \"you people just used shehnaz in whole season fck off bc\"\n",
    "]\n",
    "\n",
    "# Preprocess and tokenize the new tweets\n",
    "preprocessed_new_tweets = [preprocess_text(tweet) for tweet in new_tweets]\n",
    "new_sequences = tokenizer.texts_to_sequences(preprocessed_new_tweets)\n",
    "padded_new_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(padded_new_sequences)\n",
    "\n",
    "# Get the predicted class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels)\n",
    "classes = {\n",
    "  0: \"Neutral\",\n",
    "  1: \"Racism\",\n",
    "  2: \"Abusive/Offensive\", 3:\"Sexism\" , 4:\"Relegious Hate\" \n",
    "}\n",
    "\n",
    "# Print the predictions\n",
    "for i, tweet in enumerate(new_tweets):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Predicted label: {classes[predicted_labels[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1205703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for abuser profile identification\n",
    "\n",
    "\n",
    "csv_file = 'Data_With_Profiles.csv'\n",
    "\n",
    "# Create an empty dictionary to store the user-tweet data\n",
    "user_tweets = {}\n",
    "\n",
    "# Create an empty dictionary to store the abuser data\n",
    "abuser_profile = {}\n",
    "\n",
    "# Read the CSV file\n",
    "with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Skip header row if present\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    # Iterate through the rows of the file\n",
    "    for row in csv_reader:\n",
    "        user = row[2]\n",
    "        tweet = row[0]\n",
    "        \n",
    "        # Check if the user already exists in the dictionary\n",
    "        if user in user_tweets:\n",
    "            # Append the tweet to the existing list\n",
    "            user_tweets[user].append(tweet)\n",
    "        else:\n",
    "            # Create a new list and add the tweet\n",
    "            user_tweets[user] = [tweet]\n",
    "            \n",
    "\n",
    "# Iterate through the user_tweets dictionary and predict if the tweets are bullying or not\n",
    "for user, tweets in user_tweets.items():\n",
    "    b_tweets = 0\n",
    "    nb_tweets = 0\n",
    "    preprocessed_new_tweets = [preprocess_text(tweet) for tweet in tweets]\n",
    "    new_sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded_new_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "    # Make predictions\n",
    "    predictions = model.predict(padded_new_sequences)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    print(predicted_labels)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        if predicted_labels[i] == 0:\n",
    "            nb_tweets = nb_tweets + 1\n",
    "        elif predicted_labels[i] == 2 or predicted_labels[i] == 3:\n",
    "            b_tweets = b_tweets + 1\n",
    "    print(f\"User: {user}, No of bullying tweets: {b_tweets}, No of Non Bullying: {nb_tweets}\")\n",
    "    if nb_tweets > 1:\n",
    "        abuser_profile[user] = round((b_tweets/(nb_tweets + b_tweets)) * 100, 2)\n",
    "print(abuser_profile)\n",
    "\n",
    "busers=[]\n",
    "susers=[]\n",
    "nusers=[]\n",
    "for user in abuser_profile:\n",
    "    count = 0\n",
    "    bcount = 0\n",
    "    scount = 0\n",
    "    if abuser_profile[user] >= 60:\n",
    "        bcount = bcount + 1;\n",
    "        busers.append(user)\n",
    "    elif abuser_profile[user] < 60 and abuser_profile[user] >= 50:\n",
    "        scount = scount + 1;\n",
    "        susers.append(user)\n",
    "    elif abuser_profile[user] < 50:\n",
    "        count = count + 1\n",
    "        nusers.append(user)\n",
    "\n",
    "print(f\"No of Normal Users: {count}, No of Bullying Users: {bcount} , No of suspected Users:{scount}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
