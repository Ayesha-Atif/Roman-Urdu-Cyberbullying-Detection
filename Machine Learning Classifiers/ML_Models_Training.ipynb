{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83228c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.99      0.85      1234\n",
      "           1       1.00      0.06      0.12       139\n",
      "           2       0.81      0.66      0.73       611\n",
      "           3       1.00      0.03      0.05        75\n",
      "           4       1.00      0.07      0.12        76\n",
      "\n",
      "    accuracy                           0.77      2135\n",
      "   macro avg       0.91      0.36      0.38      2135\n",
      "weighted avg       0.80      0.77      0.71      2135\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.99      0.85      1238\n",
      "           1       0.92      0.08      0.15       147\n",
      "           2       0.83      0.65      0.72       612\n",
      "           3       1.00      0.01      0.03        69\n",
      "           4       1.00      0.07      0.14        69\n",
      "\n",
      "    accuracy                           0.77      2135\n",
      "   macro avg       0.90      0.36      0.38      2135\n",
      "weighted avg       0.80      0.77      0.72      2135\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83      1171\n",
      "           1       0.94      0.10      0.18       154\n",
      "           2       0.83      0.59      0.69       654\n",
      "           3       1.00      0.03      0.07        87\n",
      "           4       1.00      0.04      0.08        68\n",
      "\n",
      "    accuracy                           0.74      2134\n",
      "   macro avg       0.90      0.35      0.37      2134\n",
      "weighted avg       0.78      0.74      0.68      2134\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.85      1217\n",
      "           1       1.00      0.09      0.17       143\n",
      "           2       0.84      0.67      0.75       619\n",
      "           3       1.00      0.02      0.05        84\n",
      "           4       1.00      0.11      0.20        71\n",
      "\n",
      "    accuracy                           0.77      2134\n",
      "   macro avg       0.92      0.38      0.40      2134\n",
      "weighted avg       0.81      0.77      0.72      2134\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      1195\n",
      "           1       0.81      0.09      0.16       142\n",
      "           2       0.85      0.62      0.72       645\n",
      "           3       0.00      0.00      0.00        83\n",
      "           4       1.00      0.03      0.06        69\n",
      "\n",
      "    accuracy                           0.76      2134\n",
      "   macro avg       0.68      0.35      0.36      2134\n",
      "weighted avg       0.75      0.76      0.70      2134\n",
      "\n",
      "Average accuracy:  0.7602122872901983\n",
      "Average precision:  0.7894804209780493\n",
      "Average recall:  0.7602122872901983\n",
      "Average f1-measure:  0.7086703148383673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the dataset\n",
    "tweets = pd.read_csv(\"Multi_Labeled_Data.csv\",usecols=['Tweet_Text', 'Label'])\n",
    "\n",
    "# Preprocess the tweets (optional)\n",
    "def preprocess_text(text):\n",
    "    # Apply any text preprocessing steps you want, like lowercasing, stemming, etc.\n",
    "    return text.lower()\n",
    "\n",
    "preprocessed_tweets = [preprocess_text(tweet) for tweet in tweets['Tweet_Text']]\n",
    "y =tweets['Label']\n",
    "\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_tweets)\n",
    "\n",
    "# Initialize the KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Perform cross-validation\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X_tfidf):\n",
    "    X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the Naive Bayes classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision= precision_score(y_test, y_pred,average='weighted')\n",
    "    precision_scores.append(precision)\n",
    "    recall= recall_score(y_test, y_pred,average='weighted')\n",
    "    recall_scores.append(recall)\n",
    "    f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate the average accuracy score\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "print(\"Average accuracy: \", avg_accuracy)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "print(\"Average precision: \", avg_precision)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "print(\"Average recall: \", avg_recall)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "print(\"Average f1-measure: \", avg_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289f16ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92      1234\n",
      "           1       0.80      0.47      0.60       139\n",
      "           2       0.94      0.87      0.90       611\n",
      "           3       0.96      0.35      0.51        75\n",
      "           4       0.94      0.41      0.57        76\n",
      "\n",
      "    accuracy                           0.88      2135\n",
      "   macro avg       0.90      0.62      0.70      2135\n",
      "weighted avg       0.89      0.88      0.87      2135\n",
      "\n",
      "LogisticRegression Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      1238\n",
      "           1       0.86      0.59      0.70       147\n",
      "           2       0.97      0.88      0.92       612\n",
      "           3       1.00      0.30      0.47        69\n",
      "           4       0.89      0.46      0.61        69\n",
      "\n",
      "    accuracy                           0.90      2135\n",
      "   macro avg       0.92      0.65      0.73      2135\n",
      "weighted avg       0.90      0.90      0.89      2135\n",
      "\n",
      "LogisticRegression Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1171\n",
      "           1       0.92      0.61      0.73       154\n",
      "           2       0.97      0.84      0.90       654\n",
      "           3       1.00      0.39      0.56        87\n",
      "           4       0.84      0.38      0.53        68\n",
      "\n",
      "    accuracy                           0.88      2134\n",
      "   macro avg       0.91      0.64      0.73      2134\n",
      "weighted avg       0.89      0.88      0.87      2134\n",
      "\n",
      "LogisticRegression Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93      1217\n",
      "           1       0.81      0.55      0.66       143\n",
      "           2       0.97      0.89      0.93       619\n",
      "           3       1.00      0.35      0.51        84\n",
      "           4       0.94      0.42      0.58        71\n",
      "\n",
      "    accuracy                           0.89      2134\n",
      "   macro avg       0.92      0.64      0.72      2134\n",
      "weighted avg       0.90      0.89      0.88      2134\n",
      "\n",
      "LogisticRegression Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93      1195\n",
      "           1       0.88      0.64      0.74       142\n",
      "           2       0.98      0.88      0.93       645\n",
      "           3       0.91      0.47      0.62        83\n",
      "           4       0.92      0.35      0.51        69\n",
      "\n",
      "    accuracy                           0.90      2134\n",
      "   macro avg       0.91      0.67      0.74      2134\n",
      "weighted avg       0.90      0.90      0.89      2134\n",
      "\n",
      "LogisticRegression accuracy: 0.8891, Precision: 0.8959,  Recall: 0.8891,  f1-score: 0.8785\n",
      "SVC Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1234\n",
      "           1       0.89      0.73      0.81       139\n",
      "           2       0.98      0.92      0.95       611\n",
      "           3       0.92      0.73      0.81        75\n",
      "           4       0.94      0.80      0.87        76\n",
      "\n",
      "    accuracy                           0.94      2135\n",
      "   macro avg       0.93      0.84      0.88      2135\n",
      "weighted avg       0.94      0.94      0.94      2135\n",
      "\n",
      "SVC Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1238\n",
      "           1       0.86      0.84      0.85       147\n",
      "           2       0.99      0.92      0.95       612\n",
      "           3       0.93      0.58      0.71        69\n",
      "           4       0.89      0.84      0.87        69\n",
      "\n",
      "    accuracy                           0.95      2135\n",
      "   macro avg       0.92      0.84      0.87      2135\n",
      "weighted avg       0.95      0.95      0.94      2135\n",
      "\n",
      "SVC Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1171\n",
      "           1       0.93      0.83      0.88       154\n",
      "           2       0.99      0.91      0.95       654\n",
      "           3       0.96      0.74      0.83        87\n",
      "           4       0.94      0.87      0.90        68\n",
      "\n",
      "    accuracy                           0.94      2134\n",
      "   macro avg       0.95      0.87      0.90      2134\n",
      "weighted avg       0.95      0.94      0.94      2134\n",
      "\n",
      "SVC Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1217\n",
      "           1       0.84      0.80      0.82       143\n",
      "           2       0.99      0.92      0.95       619\n",
      "           3       0.98      0.69      0.81        84\n",
      "           4       0.92      0.77      0.84        71\n",
      "\n",
      "    accuracy                           0.94      2134\n",
      "   macro avg       0.93      0.84      0.88      2134\n",
      "weighted avg       0.95      0.94      0.94      2134\n",
      "\n",
      "SVC Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1195\n",
      "           1       0.91      0.89      0.90       142\n",
      "           2       1.00      0.93      0.96       645\n",
      "           3       0.91      0.82      0.86        83\n",
      "           4       0.92      0.68      0.78        69\n",
      "\n",
      "    accuracy                           0.96      2134\n",
      "   macro avg       0.94      0.87      0.90      2134\n",
      "weighted avg       0.96      0.96      0.95      2134\n",
      "\n",
      "SVC accuracy: 0.9467, Precision: 0.9475,  Recall: 0.9467,  f1-score: 0.9451\n",
      "RandomForestClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1234\n",
      "           1       0.91      0.79      0.85       139\n",
      "           2       0.96      0.94      0.95       611\n",
      "           3       0.96      0.60      0.74        75\n",
      "           4       0.99      0.87      0.92        76\n",
      "\n",
      "    accuracy                           0.95      2135\n",
      "   macro avg       0.95      0.84      0.89      2135\n",
      "weighted avg       0.95      0.95      0.95      2135\n",
      "\n",
      "RandomForestClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1238\n",
      "           1       0.90      0.86      0.88       147\n",
      "           2       0.96      0.94      0.95       612\n",
      "           3       0.96      0.67      0.79        69\n",
      "           4       0.92      0.87      0.90        69\n",
      "\n",
      "    accuracy                           0.96      2135\n",
      "   macro avg       0.94      0.87      0.90      2135\n",
      "weighted avg       0.96      0.96      0.96      2135\n",
      "\n",
      "RandomForestClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1171\n",
      "           1       0.94      0.82      0.88       154\n",
      "           2       0.98      0.92      0.95       654\n",
      "           3       0.97      0.77      0.86        87\n",
      "           4       0.86      0.88      0.87        68\n",
      "\n",
      "    accuracy                           0.95      2134\n",
      "   macro avg       0.94      0.88      0.91      2134\n",
      "weighted avg       0.95      0.95      0.95      2134\n",
      "\n",
      "RandomForestClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      1217\n",
      "           1       0.91      0.84      0.87       143\n",
      "           2       0.97      0.95      0.96       619\n",
      "           3       1.00      0.69      0.82        84\n",
      "           4       0.95      0.86      0.90        71\n",
      "\n",
      "    accuracy                           0.96      2134\n",
      "   macro avg       0.96      0.87      0.91      2134\n",
      "weighted avg       0.96      0.96      0.96      2134\n",
      "\n",
      "RandomForestClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1195\n",
      "           1       0.90      0.87      0.88       142\n",
      "           2       0.98      0.95      0.96       645\n",
      "           3       0.95      0.71      0.81        83\n",
      "           4       0.97      0.83      0.89        69\n",
      "\n",
      "    accuracy                           0.96      2134\n",
      "   macro avg       0.95      0.87      0.91      2134\n",
      "weighted avg       0.96      0.96      0.96      2134\n",
      "\n",
      "RandomForestClassifier accuracy: 0.9546, Precision: 0.9549,  Recall: 0.9546,  f1-score: 0.9532\n",
      "GradientBoostingClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1234\n",
      "           1       0.87      0.85      0.86       139\n",
      "           2       0.99      0.89      0.94       611\n",
      "           3       0.89      0.87      0.88        75\n",
      "           4       0.96      0.95      0.95        76\n",
      "\n",
      "    accuracy                           0.95      2135\n",
      "   macro avg       0.93      0.91      0.92      2135\n",
      "weighted avg       0.95      0.95      0.95      2135\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      1238\n",
      "           1       0.86      0.91      0.89       147\n",
      "           2       1.00      0.88      0.93       612\n",
      "           3       0.90      0.90      0.90        69\n",
      "           4       0.88      0.94      0.91        69\n",
      "\n",
      "    accuracy                           0.95      2135\n",
      "   macro avg       0.92      0.93      0.92      2135\n",
      "weighted avg       0.96      0.95      0.95      2135\n",
      "\n",
      "GradientBoostingClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1171\n",
      "           1       0.92      0.86      0.89       154\n",
      "           2       1.00      0.86      0.92       654\n",
      "           3       0.90      0.95      0.93        87\n",
      "           4       0.89      0.97      0.93        68\n",
      "\n",
      "    accuracy                           0.94      2134\n",
      "   macro avg       0.93      0.93      0.93      2134\n",
      "weighted avg       0.95      0.94      0.94      2134\n",
      "\n",
      "GradientBoostingClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1217\n",
      "           1       0.91      0.87      0.89       143\n",
      "           2       0.99      0.89      0.94       619\n",
      "           3       0.91      0.96      0.94        84\n",
      "           4       0.94      0.90      0.92        71\n",
      "\n",
      "    accuracy                           0.96      2134\n",
      "   macro avg       0.94      0.93      0.93      2134\n",
      "weighted avg       0.96      0.96      0.96      2134\n",
      "\n",
      "GradientBoostingClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      1195\n",
      "           1       0.89      0.88      0.88       142\n",
      "           2       1.00      0.90      0.95       645\n",
      "           3       0.89      0.95      0.92        83\n",
      "           4       0.88      0.87      0.88        69\n",
      "\n",
      "    accuracy                           0.96      2134\n",
      "   macro avg       0.92      0.92      0.92      2134\n",
      "weighted avg       0.96      0.96      0.96      2134\n",
      "\n",
      "GradientBoostingClassifier accuracy: 0.9527, Precision: 0.9544,  Recall: 0.9527,  f1-score: 0.9521\n",
      "DecisionTreeClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1234\n",
      "           1       0.90      0.88      0.89       139\n",
      "           2       0.97      0.95      0.96       611\n",
      "           3       0.87      0.81      0.84        75\n",
      "           4       0.90      0.96      0.93        76\n",
      "\n",
      "    accuracy                           0.97      2135\n",
      "   macro avg       0.93      0.92      0.92      2135\n",
      "weighted avg       0.97      0.97      0.97      2135\n",
      "\n",
      "DecisionTreeClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1238\n",
      "           1       0.87      0.95      0.91       147\n",
      "           2       0.98      0.95      0.97       612\n",
      "           3       0.85      0.81      0.83        69\n",
      "           4       0.93      0.96      0.94        69\n",
      "\n",
      "    accuracy                           0.98      2135\n",
      "   macro avg       0.92      0.93      0.93      2135\n",
      "weighted avg       0.98      0.98      0.98      2135\n",
      "\n",
      "DecisionTreeClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1171\n",
      "           1       0.94      0.88      0.91       154\n",
      "           2       0.98      0.96      0.97       654\n",
      "           3       0.90      0.89      0.89        87\n",
      "           4       0.87      0.97      0.92        68\n",
      "\n",
      "    accuracy                           0.97      2134\n",
      "   macro avg       0.93      0.94      0.94      2134\n",
      "weighted avg       0.97      0.97      0.97      2134\n",
      "\n",
      "DecisionTreeClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1217\n",
      "           1       0.92      0.90      0.91       143\n",
      "           2       0.97      0.97      0.97       619\n",
      "           3       0.84      0.83      0.84        84\n",
      "           4       0.87      0.87      0.87        71\n",
      "\n",
      "    accuracy                           0.97      2134\n",
      "   macro avg       0.92      0.91      0.92      2134\n",
      "weighted avg       0.97      0.97      0.97      2134\n",
      "\n",
      "DecisionTreeClassifier Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1195\n",
      "           1       0.91      0.87      0.89       142\n",
      "           2       0.98      0.97      0.97       645\n",
      "           3       0.82      0.88      0.85        83\n",
      "           4       0.87      0.90      0.89        69\n",
      "\n",
      "    accuracy                           0.97      2134\n",
      "   macro avg       0.91      0.92      0.92      2134\n",
      "weighted avg       0.97      0.97      0.97      2134\n",
      "\n",
      "DecisionTreeClassifier accuracy: 0.9726, Precision: 0.9727,  Recall: 0.9726,  f1-score: 0.9726\n"
     ]
    }
   ],
   "source": [
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC(kernel='linear')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "classifiers = [logreg, svm, rf, gb, dt]\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X_tfidf):\n",
    "        X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Word2Vec Technique\n",
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC(kernel='linear')\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "classifiers = [gb, dt, logreg, svm, rf]\n",
    "\n",
    "sentences = [tweet.split() for tweet in preprocessed_tweets]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=200)\n",
    "X = []\n",
    "for tweet in sentences:\n",
    "    vec = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    vec /= count\n",
    "    X.append(vec)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61603945",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [nb]\n",
    "\n",
    "sentences = [tweet.split() for tweet in preprocessed_tweets]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=200)\n",
    "X = []\n",
    "for tweet in sentences:\n",
    "    vec = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    vec /= count\n",
    "    X.append(vec)\n",
    "X = np.array(X) - np.array(X).min()\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#using FastText Word Embedding\n",
    "# Load the pre-trained FastText embeddings\n",
    "embedding_path = 'cc.hi.300.vec.gz'\n",
    "embedding_model = KeyedVectors.load_word2vec_format(embedding_path, binary=False)\n",
    "max_length=200\n",
    "\n",
    "\n",
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "nb = MultinomialNB()\n",
    "svm = SVC(kernel='linear')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Generate embeddings for the input texts\n",
    "embedding_size = embedding_model.vector_size\n",
    "input_embeddings = []\n",
    "y = tweets['Label']\n",
    "\n",
    "for text in preprocessed_tweets:\n",
    "    text_embeddings = []\n",
    "    for word in text:\n",
    "        if word in embedding_model.key_to_index:\n",
    "            text_embeddings.append(embedding_model[word])\n",
    "        else:\n",
    "            text_embeddings.append(np.zeros(embedding_size))\n",
    "    input_embeddings.append(text_embeddings)\n",
    "\n",
    "padded_embeddings = pad_sequences(input_embeddings, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=np.zeros(embedding_size))\n",
    "X = padded_embeddings.reshape(padded_embeddings.shape[0], -1)\n",
    "y = np.array(y)\n",
    "classifiers = [logreg, svm, rf, dt, gb]\n",
    "for clf in classifiers:\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=5)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    prec = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    print(f'{clf.__class__.__name__} accuracy: {acc:.4f}, precision: {prec:.4f}, recall: {recall:.4f},  f1-score: {f1:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
