{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83228c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the dataset\n",
    "tweets = pd.read_csv(\"Multi_Labeled_Data.csv\",usecols=['Tweet_Text', 'Label'])\n",
    "\n",
    "# Preprocess the tweets (optional)\n",
    "def preprocess_text(text):\n",
    "    # Apply any text preprocessing steps you want, like lowercasing, stemming, etc.\n",
    "    return text.lower()\n",
    "\n",
    "preprocessed_tweets = [preprocess_text(tweet) for tweet in tweets['Tweet_Text']]\n",
    "y =tweets['Label']\n",
    "\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_tweets)\n",
    "\n",
    "# Initialize the KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Perform cross-validation\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X_tfidf):\n",
    "    X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the Naive Bayes classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision= precision_score(y_test, y_pred,average='weighted')\n",
    "    precision_scores.append(precision)\n",
    "    recall= recall_score(y_test, y_pred,average='weighted')\n",
    "    recall_scores.append(recall)\n",
    "    f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate the average accuracy score\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "print(\"Average accuracy: \", avg_accuracy)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "print(\"Average precision: \", avg_precision)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "print(\"Average recall: \", avg_recall)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "print(\"Average f1-measure: \", avg_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC(kernel='linear')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "classifiers = [logreg, svm, rf, gb, dt]\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X_tfidf):\n",
    "        X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Word2Vec Technique\n",
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC(kernel='linear')\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "classifiers = [gb, dt, logreg, svm, rf]\n",
    "\n",
    "sentences = [tweet.split() for tweet in preprocessed_tweets]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=200)\n",
    "X = []\n",
    "for tweet in sentences:\n",
    "    vec = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    vec /= count\n",
    "    X.append(vec)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61603945",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [nb]\n",
    "\n",
    "sentences = [tweet.split() for tweet in preprocessed_tweets]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=200)\n",
    "X = []\n",
    "for tweet in sentences:\n",
    "    vec = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    vec /= count\n",
    "    X.append(vec)\n",
    "X = np.array(X) - np.array(X).min()\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Perform cross-validation\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        # Train the Naive Bayes classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision= precision_score(y_test, y_pred,average='weighted')\n",
    "        precision_scores.append(precision)\n",
    "        recall= recall_score(y_test, y_pred,average='weighted')\n",
    "        recall_scores.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "        print(f'{clf.__class__.__name__} Classification Report: ')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate the average accuracy score\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "#     print(\"Average accuracy: \", avg_accuracy)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "#     print(\"Average precision: \", avg_precision)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "#     print(\"Average recall: \", avg_recall)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "#     print(\"Average f1-measure: \", avg_f1)\n",
    "    print(f'{clf.__class__.__name__} accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f},  Recall: {avg_recall:.4f},  f1-score: {avg_f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#using FastText Word Embedding\n",
    "# Load the pre-trained FastText embeddings\n",
    "embedding_path = 'cc.hi.300.vec.gz'\n",
    "embedding_model = KeyedVectors.load_word2vec_format(embedding_path, binary=False)\n",
    "max_length=200\n",
    "\n",
    "\n",
    "# Define the classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "nb = MultinomialNB()\n",
    "svm = SVC(kernel='linear')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Generate embeddings for the input texts\n",
    "embedding_size = embedding_model.vector_size\n",
    "input_embeddings = []\n",
    "y = tweets['Label']\n",
    "\n",
    "for text in preprocessed_tweets:\n",
    "    text_embeddings = []\n",
    "    for word in text:\n",
    "        if word in embedding_model.key_to_index:\n",
    "            text_embeddings.append(embedding_model[word])\n",
    "        else:\n",
    "            text_embeddings.append(np.zeros(embedding_size))\n",
    "    input_embeddings.append(text_embeddings)\n",
    "\n",
    "padded_embeddings = pad_sequences(input_embeddings, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=np.zeros(embedding_size))\n",
    "X = padded_embeddings.reshape(padded_embeddings.shape[0], -1)\n",
    "y = np.array(y)\n",
    "classifiers = [logreg, svm, rf, dt, gb]\n",
    "for clf in classifiers:\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=5)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    prec = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    print(f'{clf.__class__.__name__} accuracy: {acc:.4f}, precision: {prec:.4f}, recall: {recall:.4f},  f1-score: {f1:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
